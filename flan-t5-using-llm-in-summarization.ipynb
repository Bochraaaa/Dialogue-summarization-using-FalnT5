{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-10T18:38:44.917997Z","iopub.execute_input":"2023-10-10T18:38:44.918337Z","iopub.status.idle":"2023-10-10T18:38:56.653517Z","shell.execute_reply.started":"2023-10-10T18:38:44.918310Z","shell.execute_reply":"2023-10-10T18:38:56.652046Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torchdata\n","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:38:56.656226Z","iopub.execute_input":"2023-10-10T18:38:56.656710Z","iopub.status.idle":"2023-10-10T18:39:07.052114Z","shell.execute_reply.started":"2023-10-10T18:38:56.656666Z","shell.execute_reply":"2023-10-10T18:39:07.050759Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchdata in /opt/conda/lib/python3.10/site-packages (0.6.0)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata) (1.26.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchdata) (2.31.0)\nRequirement already satisfied: torch==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torchdata) (2.0.0+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchdata) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchdata) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchdata) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchdata) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchdata) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.0->torchdata) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0->torchdata) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:39:07.053797Z","iopub.execute_input":"2023-10-10T18:39:07.054950Z","iopub.status.idle":"2023-10-10T18:39:17.216756Z","shell.execute_reply.started":"2023-10-10T18:39:07.054890Z","shell.execute_reply":"2023-10-10T18:39:17.215413Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install datasets","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:39:17.220161Z","iopub.execute_input":"2023-10-10T18:39:17.220585Z","iopub.status.idle":"2023-10-10T18:39:27.854744Z","shell.execute_reply.started":"2023-10-10T18:39:17.220553Z","shell.execute_reply":"2023-10-10T18:39:27.853413Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (9.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"you should either use  pandas==1.2.5 or datase=2.11","metadata":{}},{"cell_type":"code","source":"pip install pandas==1.2.5","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:39:27.857287Z","iopub.execute_input":"2023-10-10T18:39:27.857866Z","iopub.status.idle":"2023-10-10T18:50:35.161217Z","shell.execute_reply.started":"2023-10-10T18:39:27.857814Z","shell.execute_reply":"2023-10-10T18:50:35.159641Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting pandas==1.2.5\n  Downloading pandas-1.2.5.tar.gz (5.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas==1.2.5) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas==1.2.5) (2023.3)\nRequirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.10/site-packages (from pandas==1.2.5) (1.23.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas==1.2.5) (1.16.0)\nBuilding wheels for collected packages: pandas\n  Building wheel for pandas (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pandas: filename=pandas-1.2.5-cp310-cp310-linux_x86_64.whl size=11024661 sha256=5db1cb2aaede79dc2133f8879c1fb9f00452eac1337147514882b9167ce33d34\n  Stored in directory: /root/.cache/pip/wheels/80/c4/45/fbb3bb8c610988624e1a0cb4c55ac7409fba8c9b3fbaadcd9e\nSuccessfully built pandas\nInstalling collected packages: pandas\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.0.3\n    Uninstalling pandas-2.0.3:\n      Successfully uninstalled pandas-2.0.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2023.621.222118 requires jupyter-server~=1.16, but you have jupyter-server 2.6.0 which is incompatible.\nesda 2.5.0 requires pandas>1.4, but you have pandas 1.2.5 which is incompatible.\nfeaturetools 1.27.0 requires pandas>=1.5.0, but you have pandas 1.2.5 which is incompatible.\nfitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.2.5 which is incompatible.\nmizani 0.10.0 requires pandas>=1.3.5, but you have pandas 1.2.5 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nplotnine 0.10.1 requires pandas>=1.3.5, but you have pandas 1.2.5 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\nwfdb 4.1.2 requires pandas>=1.3.0, but you have pandas 1.2.5 which is incompatible.\nwoodwork 0.26.0 requires pandas>=1.4.3, but you have pandas 1.2.5 which is incompatible.\nxarray 2023.8.0 requires pandas>=1.4, but you have pandas 1.2.5 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pandas-1.2.5\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchdata\nimport transformers\nimport datasets\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:50:35.163277Z","iopub.execute_input":"2023-10-10T18:50:35.163715Z","iopub.status.idle":"2023-10-10T18:50:41.435748Z","shell.execute_reply.started":"2023-10-10T18:50:35.163673Z","shell.execute_reply":"2023-10-10T18:50:41.433665Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:50:41.436956Z","iopub.execute_input":"2023-10-10T18:50:41.437470Z","iopub.status.idle":"2023-10-10T18:50:42.068679Z","shell.execute_reply.started":"2023-10-10T18:50:41.437441Z","shell.execute_reply":"2023-10-10T18:50:42.067357Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"huggingface_dataset_name= \"knkarthick/dialogsum\"\ndataset= load_dataset(huggingface_dataset_name)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:50:42.070038Z","iopub.execute_input":"2023-10-10T18:50:42.070333Z","iopub.status.idle":"2023-10-10T18:50:44.371321Z","shell.execute_reply.started":"2023-10-10T18:50:42.070310Z","shell.execute_reply":"2023-10-10T18:50:44.369980Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/csv/knkarthick--dialogsum-1aed23a5f481e688/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e0fb68b93cc4c83b9fe4e9e1d41820e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c9223203ed4605bbdfa6b3cc6fb1a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37a2162315a04086a16cfd261a484284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8fef2743b2e4982a37a1dfd8680faf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d26309d760a46a59777b2808c95e7d2"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/knkarthick--dialogsum-1aed23a5f481e688/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"556706270add41fb8e76bf19573a4fcb"}},"metadata":{}}]},{"cell_type":"markdown","source":"# **Summarize Dialogue without Prompt Engineering**","metadata":{}},{"cell_type":"markdown","source":"Print a couple of  dialogues with thier baseline summaries","metadata":{}},{"cell_type":"code","source":"example_indices=[20,100]\ndash_line  = '-'.join('' for x in range (100))\nfor i, index in enumerate(example_indices) : \n    print(dash_line)\n    print('example', i+1)\n    print(dash_line)\n    print('INPUT DIALOGUE:')\n    print(dataset['test'][index]['dialogue'])\n    print(dash_line)\n    print('BASELINE HUMAN SUMMARY')\n    print(dataset['test'][index]['summary'])\n    print(dash_line)\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:53:28.417936Z","iopub.execute_input":"2023-10-10T18:53:28.418302Z","iopub.status.idle":"2023-10-10T18:53:28.426959Z","shell.execute_reply.started":"2023-10-10T18:53:28.418276Z","shell.execute_reply":"2023-10-10T18:53:28.425487Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nexample 1\n---------------------------------------------------------------------------------------------------\nINPUT DIALOGUE:\n#Person1#: What's wrong with you? Why are you scratching so much?\n#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n#Person1#: Let me have a look. Whoa! Get away from me!\n#Person2#: What's wrong?\n#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY\n#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n---------------------------------------------------------------------------------------------------\n\n---------------------------------------------------------------------------------------------------\nexample 2\n---------------------------------------------------------------------------------------------------\nINPUT DIALOGUE:\n#Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n#Person2#: What was the problem that time?\n#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n#Person2#: I'm not so sure about that.\n#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY\n#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n---------------------------------------------------------------------------------------------------\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To perform encoding/decoding, you need to work with text in a tokenized form.\n\n**Tokenization:**\nis the process of splitting the text into smaller units that can be processed by the LLM (in this case FLAN-T5)\nLoad the  FLAN-T5 model, creating an instance of the AutoModelForSeq2SeqLM class with .from_pretrained() method","metadata":{}},{"cell_type":"code","source":"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:53:43.402416Z","iopub.execute_input":"2023-10-10T18:53:43.402777Z","iopub.status.idle":"2023-10-10T18:54:03.703714Z","shell.execute_reply.started":"2023-10-10T18:53:43.402752Z","shell.execute_reply":"2023-10-10T18:54:03.702385Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ae230105cc47bf8260cfe82ef6c211"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b656899ec3846a4a4393d6a8979be11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aab7ba8b80fe46f0bc76bc4f1e3679f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec05f0b462e4f6f80234580df0f53cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd5cef21e0d4e1dbac3ab2ffd06cdff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0611447c815450dae3560b62dbe16b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"184d692748d34083940e9a95d708b96e"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Test the tokenizer encoding and decoding s simple sentence**","metadata":{}},{"cell_type":"code","source":"sentence =\"what are you doing?\"\nsentence_encoded = tokenizer(sentence, return_tensors='pt')\nsentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0],\n                                    skip_special_tokens=True)\n\nprint(\"Encoded sentence:\")\nprint(sentence_encoded[\"input_ids\"][0])\nprint(\"\\nDecoded sentence\")\nprint(sentence_decoded)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:54:11.268030Z","iopub.execute_input":"2023-10-10T18:54:11.268425Z","iopub.status.idle":"2023-10-10T18:54:11.281121Z","shell.execute_reply.started":"2023-10-10T18:54:11.268395Z","shell.execute_reply":"2023-10-10T18:54:11.280258Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Encoded sentence:\ntensor([125,  33,  25, 692,  58,   1])\n\nDecoded sentence\nwhat are you doing?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Dialogue summarization using FLAN-T5 without prompt engineering","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices): \n    dialogue= dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n    inputs = tokenizer(dialogue, return_tensors='pt')\n    output =tokenizer.decode(model.generate(inputs[\"input_ids\"],\n                                          max_new_tokens = 50,)\n                            [0], \n                             skip_special_tokens= True)\n    print(dash_line)\n    print('example', i+1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{dialogue}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print(dash_line)\n    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINERING:\\n{output}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:54:16.265482Z","iopub.execute_input":"2023-10-10T18:54:16.265883Z","iopub.status.idle":"2023-10-10T18:54:20.260403Z","shell.execute_reply.started":"2023-10-10T18:54:16.265855Z","shell.execute_reply":"2023-10-10T18:54:20.259091Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nexample 1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n#Person1#: What's wrong with you? Why are you scratching so much?\n#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n#Person1#: Let me have a look. Whoa! Get away from me!\n#Person2#: What's wrong?\n#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - WITHOUT PROMPT ENGINERING:\nPerson1#: I'm scratching so much. I can't stand it anymore. I think I may be coming down with something. I feel lightheaded and weak.\n\n---------------------------------------------------------------------------------------------------\nexample 2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n#Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n#Person2#: What was the problem that time?\n#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n#Person2#: I'm not so sure about that.\n#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - WITHOUT PROMPT ENGINERING:\n#Person1#: I'm sorry, but I'm not sure what Jason and Laura are doing.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Prompt engineering :**\n is an act of a human changing the prompt(input) to improve the response for a given task.","metadata":{}},{"cell_type":"markdown","source":"**Zero Shot Inference with an Instruction Prompt :**\n\nIn order to instruct the model to perform a task (dialogue summarization) you can take the dialogue and convert it into an instruction prompt. This is often called Zero Shot Inference.","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices): \n    dialogue= dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n    \n    prompt =f\"\"\"\n    \nSummarize the following conversation.\n\n{dialogue}\n\nsummary: \n    \"\"\"\n    inputs= tokenizer(prompt, return_tensors='pt')\n   \n    output =tokenizer.decode(model.generate(inputs[\"input_ids\"],\n                                          max_new_tokens = 50,)\n                            [0], \n                             skip_special_tokens= True)\n    print(dash_line)\n    print('Example', i+1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{prompt}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print(dash_line)\n    print(f'MODEL GENERATION - Zero shot:\\n{output}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:54:56.262363Z","iopub.execute_input":"2023-10-10T18:54:56.262712Z","iopub.status.idle":"2023-10-10T18:54:58.886784Z","shell.execute_reply.started":"2023-10-10T18:54:56.262686Z","shell.execute_reply":"2023-10-10T18:54:58.885644Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample 1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\n    \nSummarize the following conversation.\n\n#Person1#: What's wrong with you? Why are you scratching so much?\n#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n#Person1#: Let me have a look. Whoa! Get away from me!\n#Person2#: What's wrong?\n#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n\nsummary: \n    \n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - Zero shot:\nPerson1 is scratching a lot.\n\n---------------------------------------------------------------------------------------------------\nExample 2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\n    \nSummarize the following conversation.\n\n#Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n#Person2#: What was the problem that time?\n#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n#Person2#: I'm not so sure about that.\n#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n\nsummary: \n    \n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - Zero shot:\nThe two of them are going to try to figure out how to express their feelings.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for i, inedex in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary=dataset['test'][index]['summary']\n    \n    prompt =f\"\"\"\nDialogue\n{dialogue}\nwhat wasgoing on ?\n    \"\"\"\n    inputs = tokenizer(prompt, return_tensors='pt')\n    output = tokenizer.decode(\n    model.generate(\n    inputs[\"input_ids\"],\n    max_new_tokens=50,)[0],\n    skip_special_tokens = True)\n    \n    print(dash_line)\n    print('example', i+1)\n    print(dash_line)\n    print(f'Imput promt:\\n{prompt}')\n    print(dash_line)\n    print(f'Baseline Human Summary:\\n{summary}\\n')\n    print(dash_line)\n    print(f'Model Generation Zero shot:\\n{output}\\n')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:55:30.190086Z","iopub.execute_input":"2023-10-10T18:55:30.190479Z","iopub.status.idle":"2023-10-10T18:55:32.158709Z","shell.execute_reply.started":"2023-10-10T18:55:30.190452Z","shell.execute_reply":"2023-10-10T18:55:32.156686Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nexample 1\n---------------------------------------------------------------------------------------------------\nImput promt:\n\nDialogue\n#Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n#Person2#: What was the problem that time?\n#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n#Person2#: I'm not so sure about that.\n#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\nwhat wasgoing on ?\n    \n---------------------------------------------------------------------------------------------------\nBaseline Human Summary:\n#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n\n---------------------------------------------------------------------------------------------------\nModel Generation Zero shot:\nThe feeling was all wrong, Mike.\n\n---------------------------------------------------------------------------------------------------\nexample 2\n---------------------------------------------------------------------------------------------------\nImput promt:\n\nDialogue\n#Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n#Person2#: What was the problem that time?\n#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n#Person2#: I'm not so sure about that.\n#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\nwhat wasgoing on ?\n    \n---------------------------------------------------------------------------------------------------\nBaseline Human Summary:\n#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n\n---------------------------------------------------------------------------------------------------\nModel Generation Zero shot:\nThe feeling was all wrong, Mike.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Summarize Dialogue With One Shot & Few Shot Inference**","metadata":{}},{"cell_type":"markdown","source":"One Shot and Few Shot are the practice of providing an LLM with either one or more full examples of prompt-response pairs that match your task before your actual prompt that you want completed. This called in context learning and puts your model into a state that understands your specific task","metadata":{}},{"cell_type":"code","source":"def make_prompt(example_indices_full, example_index_to_summarize):\n    prompt=''\n    for index in example_indices_full : \n        dialogue = dataset['test'][index]['dialogue']\n        summary = dataset['test'][index]['summary']\n        prompt +=f\"\"\"\nDialogue :\n{dialogue}\nwhat was going on?\n{summary}\n        \"\"\"\n        dialogue =dataset['test'][example_index_to_summarize]['dialogue']\n        prompt +=f\"\"\"\nDialogue : \n{dialogue}\nwhat was going on?\n        \"\"\"\n        return(prompt)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:55:37.360220Z","iopub.execute_input":"2023-10-10T18:55:37.361316Z","iopub.status.idle":"2023-10-10T18:55:37.367709Z","shell.execute_reply.started":"2023-10-10T18:55:37.361262Z","shell.execute_reply":"2023-10-10T18:55:37.366489Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# **One Shot Inference**","metadata":{}},{"cell_type":"code","source":"example_indices_full= [10]\nexample_index_to_summarize = 512\none_shot_prompt = make_prompt(example_indices_full,example_index_to_summarize)\nprint(one_shot_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:58:07.881733Z","iopub.execute_input":"2023-10-10T18:58:07.882166Z","iopub.status.idle":"2023-10-10T18:58:07.889657Z","shell.execute_reply.started":"2023-10-10T18:58:07.882136Z","shell.execute_reply":"2023-10-10T18:58:07.888482Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\nDialogue :\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nwhat was going on?\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n        \nDialogue : \n#Person1#: Excuse me. I'm a stranger here and lost my way.\n#Person2#: Can I help you?\n#Person1#: Sure. Can I get to the central department stall this way?\n#Person2#: uh. . . Yes. Turn right, then take the second turning on your left.\n#Person1#: Is it far?\n#Person2#: It's about fifteen minutes'walk. That's all.\n#Person1#: And do you know where the national bank is?\n#Person2#: Yes. It's on this street. Keep walking for two blocks and it's on the corner on the right.\n#Person1#: Thanks very much.\n#Person2#: You are welcome. Good luck.\n#Person1#: Thank you. I won't miss it.\nwhat was going on?\n        \n","output_type":"stream"}]},{"cell_type":"code","source":"summary = dataset['test'][example_index_to_summarize]['summary']\niputs = tokenizer(one_shot_prompt, return_tensors='pt')\nouput =tokenizer.decode(\nmodel.generate(\ninputs[\"input_ids\"], \nmax_new_tokens=50,)\n[0], \nskip_special_tokens =True)\nprint(dash_line)\nprint(f'Baseline Human Summary:\\n{summary}\\n')\nprint(dash_line)\nprint(f'Model Generation -- one shot:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T18:58:13.337152Z","iopub.execute_input":"2023-10-10T18:58:13.337574Z","iopub.status.idle":"2023-10-10T18:58:14.425854Z","shell.execute_reply.started":"2023-10-10T18:58:13.337544Z","shell.execute_reply":"2023-10-10T18:58:14.424683Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBaseline Human Summary:\n#Person1# asks #Person2# the way to the central department stall and the national bank.\n\n---------------------------------------------------------------------------------------------------\nModel Generation -- one shot:\nThe feeling was all wrong, Mike.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Few Shot Prompt**","metadata":{}},{"cell_type":"code","source":"example_indices_full= [55,100,512]\nexample_index_to_summarize = 512\nfew_shot_prompt = make_prompt(example_indices_full,example_index_to_summarize)\nprint(few_shot_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:06:31.705143Z","iopub.execute_input":"2023-10-10T19:06:31.705535Z","iopub.status.idle":"2023-10-10T19:06:31.713306Z","shell.execute_reply.started":"2023-10-10T19:06:31.705507Z","shell.execute_reply":"2023-10-10T19:06:31.712374Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"\nDialogue :\n#Person1#: Is anybody in?\n#Person2#: How can I help you?\n#Person1#: I have a headache.\n#Person2#: Let me take your temperature with a thermometer.\n#Person1#: OK.\n#Person2#: I think you have a small fever.\n#Person1#: I thought so. I felt dizzy this morning.\n#Person2#: You should've called in sick! Next time, have either of your parents call the school office.\nwhat was going on?\n#Person2# finds that #Person1# has a fever and says #Person1# should've called in sick earlier.\n        \nDialogue : \n#Person1#: Excuse me. I'm a stranger here and lost my way.\n#Person2#: Can I help you?\n#Person1#: Sure. Can I get to the central department stall this way?\n#Person2#: uh. . . Yes. Turn right, then take the second turning on your left.\n#Person1#: Is it far?\n#Person2#: It's about fifteen minutes'walk. That's all.\n#Person1#: And do you know where the national bank is?\n#Person2#: Yes. It's on this street. Keep walking for two blocks and it's on the corner on the right.\n#Person1#: Thanks very much.\n#Person2#: You are welcome. Good luck.\n#Person1#: Thank you. I won't miss it.\nwhat was going on?\n        \n","output_type":"stream"}]},{"cell_type":"code","source":"summary = dataset['test'][example_index_to_summarize]['summary']\ninputs =tokenizer(few_shot_prompt,return_tensors='pt')\noutput=tokenizer.decode(\nmodel.generate(\ninputs[\"input_ids\"],\nmax_new_tokens = 50,)\n[0],\nskip_special_tokens =True)\nprint(dash_line)\nprint(f'Baseline Human Summary:\\n{summary}\\n')\nprint(dash_line)\nprint(f'Model Generation -- few shot:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:06:36.191461Z","iopub.execute_input":"2023-10-10T19:06:36.191857Z","iopub.status.idle":"2023-10-10T19:06:38.470146Z","shell.execute_reply.started":"2023-10-10T19:06:36.191831Z","shell.execute_reply":"2023-10-10T19:06:38.469016Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBaseline Human Summary:\n#Person1# asks #Person2# the way to the central department stall and the national bank.\n\n---------------------------------------------------------------------------------------------------\nModel Generation -- few shot:\n#Person1# is a stranger and lost his way. #Person2# will help him.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Generative Configuration Paramaters for Inference**","metadata":{}},{"cell_type":"markdown","source":"You can change the configuration parameters of the generate()  methode to see a different output from the LLM.\nPutting the parameter do_sample = True, you activate various decoding strategies which influance the next token from the probability distribution over the entier vocabulary. You can adjust the outputs changing temperature and ther parameters( top_k, top_p)","metadata":{}},{"cell_type":"code","source":"generation_config = GenerationConfig(max_new_tokens=50)\n#generation_config = GenrationConfig(max_new_tokens = 10)\n#generation_config = GenerationConfig(max_new_tokens = 50, do_sample = True, temperature=0.1)\n#generation_config = GenerationConfig(max_new_tokens=50, do_sample= True, temperature=0.5)\n#generation_config= GenerationConfig(max_new_tokens = 50,do_sample =True, temperature = 1.0)\ninputs = tokenizer(few_shot_prompt, return_tensors ='pt')\noutput=tokenizer.decode(\nmodel.generate(\ninputs[\"input_ids\"],\ngeneration_config=generation_config,)\n[0],\nskip_special_tokens= True)\nprint(dash_line)\nprint(f'Model Generation few_shot:\\n{output}')\nprint(dash_line)\nprint(f'Baseline Human summary : \\n{summary}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:06:58.972633Z","iopub.execute_input":"2023-10-10T19:06:58.973015Z","iopub.status.idle":"2023-10-10T19:07:01.007837Z","shell.execute_reply.started":"2023-10-10T19:06:58.972987Z","shell.execute_reply":"2023-10-10T19:07:01.006696Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nModel Generation few_shot:\n#Person1# is a stranger and lost his way. #Person2# will help him.\n---------------------------------------------------------------------------------------------------\nBaseline Human summary : \n#Person1# asks #Person2# the way to the central department stall and the national bank.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#generation_config = GenerationConfig(max_new_tokens=50)\n#generation_config = GenrationConfig(max_new_tokens = 10)\ngeneration_config = GenerationConfig(max_new_tokens = 50, do_sample = True, temperature=0.1)\n#generation_config = GenerationConfig(max_new_tokens=50, do_sample= True, temperature=0.5)\n#generation_config= GenerationConfig(max_new_tokens = 50,do_sample =True, temperature = 1.0)\ninputs = tokenizer(few_shot_prompt, return_tensors ='pt')\noutput=tokenizer.decode(\nmodel.generate(\ninputs[\"input_ids\"],\ngeneration_config=generation_config,)\n[0],\nskip_special_tokens= True)\nprint(dash_line)\nprint(f'Model Generation few_shot:\\n{output}')\nprint(dash_line)\nprint(f'Baseline Human summary : \\n{summary}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:07:31.019448Z","iopub.execute_input":"2023-10-10T19:07:31.019830Z","iopub.status.idle":"2023-10-10T19:07:32.998151Z","shell.execute_reply.started":"2023-10-10T19:07:31.019803Z","shell.execute_reply":"2023-10-10T19:07:32.996953Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nModel Generation few_shot:\n#Person1# is a stranger and lost his way. #Person2# will help him.\n---------------------------------------------------------------------------------------------------\nBaseline Human summary : \n#Person1# asks #Person2# the way to the central department stall and the national bank.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#generation_config = GenerationConfig(max_new_tokens=50)\n#generation_config = GenrationConfig(max_new_tokens = 10)\n#generation_config = GenerationConfig(max_new_tokens = 50, do_sample = True, temperature=0.1)\ngeneration_config = GenerationConfig(max_new_tokens=50, do_sample= True, temperature=0.5)\n#generation_config= GenerationConfig(max_new_tokens = 50,do_sample =True, temperature = 1.0)\ninputs = tokenizer(few_shot_prompt, return_tensors ='pt')\noutput=tokenizer.decode(\nmodel.generate(\ninputs[\"input_ids\"],\ngeneration_config=generation_config,)\n[0],\nskip_special_tokens= True)\nprint(dash_line)\nprint(f'Model Generation few_shot:\\n{output}')\nprint(dash_line)\nprint(f'Baseline Human summary : \\n{summary}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:07:33.000289Z","iopub.execute_input":"2023-10-10T19:07:33.000617Z","iopub.status.idle":"2023-10-10T19:07:34.815517Z","shell.execute_reply.started":"2023-10-10T19:07:33.000590Z","shell.execute_reply":"2023-10-10T19:07:34.814576Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nModel Generation few_shot:\n#Person1# needs help from Person2 to get to the central department stall.\n---------------------------------------------------------------------------------------------------\nBaseline Human summary : \n#Person1# asks #Person2# the way to the central department stall and the national bank.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#generation_config = GenerationConfig(max_new_tokens=50)\ngeneration_config = GenerationConfig(max_new_tokens = 10)\n#generation_config = GenerationConfig(max_new_tokens = 50, do_sample = True, temperature=0.1)\n#generation_config = GenerationConfig(max_new_tokens=50, do_sample= True, temperature=0.5)\n#generation_config= GenerationConfig(max_new_tokens = 50,do_sample =True, temperature = 1.0)\ninputs = tokenizer(few_shot_prompt, return_tensors ='pt')\noutput=tokenizer.decode(\nmodel.generate(\ninputs[\"input_ids\"],\ngeneration_config=generation_config,)\n[0],\nskip_special_tokens= True)\nprint(dash_line)\nprint(f'Model Generation few_shot:\\n{output}')\nprint(dash_line)\nprint(f'Baseline Human summary : \\n{summary}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:07:34.817398Z","iopub.execute_input":"2023-10-10T19:07:34.817807Z","iopub.status.idle":"2023-10-10T19:07:36.155773Z","shell.execute_reply.started":"2023-10-10T19:07:34.817771Z","shell.execute_reply":"2023-10-10T19:07:36.154835Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nModel Generation few_shot:\n#Person1# is a stranger and\n---------------------------------------------------------------------------------------------------\nBaseline Human summary : \n#Person1# asks #Person2# the way to the central department stall and the national bank.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#generation_config = GenerationConfig(max_new_tokens=50)\n#generation_config = GenerationConfig(max_new_tokens = 10)\ngeneration_config = GenerationConfig(max_new_tokens = 10, do_sample = True, temperature=0.1)\n#generation_config = GenerationConfig(max_new_tokens=50, do_sample= True, temperature=0.5)\n#generation_config= GenerationConfig(max_new_tokens = 50,do_sample =True, temperature = 1.0)\ninputs = tokenizer(few_shot_prompt, return_tensors ='pt')\noutput=tokenizer.decode(\nmodel.generate(\ninputs[\"input_ids\"],\ngeneration_config=generation_config,)\n[0],\nskip_special_tokens= True)\nprint(dash_line)\nprint(f'Model Generation few_shot:\\n{output}')\nprint(dash_line)\nprint(f'Baseline Human summary : \\n{summary}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:07:36.157890Z","iopub.execute_input":"2023-10-10T19:07:36.158966Z","iopub.status.idle":"2023-10-10T19:07:37.515710Z","shell.execute_reply.started":"2023-10-10T19:07:36.158900Z","shell.execute_reply":"2023-10-10T19:07:37.514309Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nModel Generation few_shot:\n#Person1# is a stranger and\n---------------------------------------------------------------------------------------------------\nBaseline Human summary : \n#Person1# asks #Person2# the way to the central department stall and the national bank.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#generation_config = GenerationConfig(max_new_tokens=50)\n#generation_config = GenerationConfig(max_new_tokens = 10)\ngeneration_config = GenerationConfig(max_new_tokens = 100, do_sample = True, temperature=0.1)\n#generation_config = GenerationConfig(max_new_tokens=50, do_sample= True, temperature=0.5)\n#generation_config= GenerationConfig(max_new_tokens = 50,do_sample =True, temperature = 1.0)\ninputs = tokenizer(few_shot_prompt, return_tensors ='pt')\noutput=tokenizer.decode(\nmodel.generate(\ninputs[\"input_ids\"],\ngeneration_config=generation_config,)\n[0],\nskip_special_tokens= True)\nprint(dash_line)\nprint(f'Model Generation few_shot:\\n{output}')\nprint(dash_line)\nprint(f'Baseline Human summary : \\n{summary}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T19:07:37.517834Z","iopub.execute_input":"2023-10-10T19:07:37.519107Z","iopub.status.idle":"2023-10-10T19:07:39.475158Z","shell.execute_reply.started":"2023-10-10T19:07:37.519060Z","shell.execute_reply":"2023-10-10T19:07:39.473986Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nModel Generation few_shot:\n#Person1# is a stranger and lost his way. #Person2# will help him.\n---------------------------------------------------------------------------------------------------\nBaseline Human summary : \n#Person1# asks #Person2# the way to the central department stall and the national bank.\n\n","output_type":"stream"}]}]}